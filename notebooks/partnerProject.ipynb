{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reagenhuskey/cs290/blob/main/notebooks/partnerProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GlTPR22lsVXO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attribute Selection Method"
      ],
      "metadata": {
        "id": "CNvW_jBosXtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "bfj_nvUUsZ24"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the entropy for a categorical feature\n",
        "def categorical_entropy(df, target, feature):\n",
        "    entropyList = []  # List to store entropy values\n",
        "    uniqueVals = df[feature].unique()  # Get unique values for the feature\n",
        "    overall = len(df)  # Total number of rows\n",
        "    for val in uniqueVals:\n",
        "        entropy = 0  # Initialize entropy for each unique value\n",
        "        subset_size = len(df[df[feature] == val])  # Size of subset for the current value\n",
        "        weight = subset_size / overall  # Weight of the subset\n",
        "        props = df[df[feature] == val][target].value_counts(normalize=True)  # Get the proportion of target values\n",
        "        for p in props:\n",
        "          entropy -= weight * (p * math.log2(p))  # Calculate entropy for the current value\n",
        "          entropyList.append(entropy)  # Append entropy value to the list\n",
        "    return min(entropyList)  # Return the minimum entropy value"
      ],
      "metadata": {
        "id": "QQBI9p6rq8j9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Gini index for a categorical feature\n",
        "def categorical_gini(df, target, feature):\n",
        "    giniList = []  # List to store Gini values\n",
        "    uniqueVals = df[feature].unique()  # Get unique values for the feature\n",
        "    overall = len(df)  # Total number of rows\n",
        "    for val in uniqueVals:\n",
        "        subset_size = len(df[df[feature] == val])  # Size of subset for the current value\n",
        "        weight = subset_size / overall  # Weight of the subset\n",
        "        props = df[df[feature] == val][target].value_counts(normalize=True)  # Get the proportion of target values\n",
        "        gini = 1 - np.sum(np.square(props))  # Calculate Gini for the current value\n",
        "        giniList.append(weight * gini)  # Append weighted Gini value to the list\n",
        "    return min(giniList)  # Return the minimum Gini value"
      ],
      "metadata": {
        "id": "VcFrXMS-rCs0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the entropy for a quantitative feature\n",
        "def quantitative_entropy(df, target, feature):\n",
        "    entropyDict = {} # Dict to store entropy values as vals & split points as keys\n",
        "    uniqueVals = np.sort(df[feature].unique())  # Sorted unique values for the feature\n",
        "    overall = len(df)  # Total number of rows\n",
        "    for val in uniqueVals:\n",
        "        entropy = 0  # Initialize entropy for each unique value\n",
        "        # Split the data into two subsets: <= val and > val\n",
        "        left = df[df[feature] <= val][[feature, target]]\n",
        "        right = df[df[feature] > val][[feature, target]]\n",
        "\n",
        "        # Calculate the entropy for the left subset\n",
        "        props_left = left[target].value_counts(normalize=True)\n",
        "        weight_left = len(left) / overall\n",
        "        for prop in props_left:\n",
        "            if prop > 0:\n",
        "                entropy -= weight_left * prop * math.log2(prop)\n",
        "\n",
        "        # Calculate the entropy for the right subset\n",
        "        props_right = right[target].value_counts(normalize=True)\n",
        "        weight_right = len(right) / overall\n",
        "        for prop in props_right:\n",
        "            if prop > 0:\n",
        "                entropy -= weight_right * prop * math.log2(prop)\n",
        "\n",
        "        entropyDict[val] = entropy\n",
        "\n",
        "    best_split = min(entropyDict, key=entropyDict.get) # Find the corresponding split point\n",
        "    min_entropy = entropyDict[best_split]  # Get the minimum entropy value\n",
        "\n",
        "    return min_entropy, best_split\n"
      ],
      "metadata": {
        "id": "YMj_4LAmrbDo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Gini index for a quantitative feature\n",
        "def quantitative_gini(df, target, feature):\n",
        "    giniDict = {}  # Dictionary to store Gini values and split points.\n",
        "    uniqueVals = np.sort(df[feature].unique())  # Sorted unique values for the feature\n",
        "    overall = len(df)  # Total number of rows\n",
        "    for val in uniqueVals:\n",
        "        gini = 0  # Initialize Gini index for each unique value\n",
        "        # Split the data into two subsets: <= val and > val\n",
        "        left = df[df[feature] <= val][[feature, target]]\n",
        "        right = df[df[feature] > val][[feature, target]]\n",
        "\n",
        "        # Calculate the Gini for the left subset\n",
        "        props_left = left[target].value_counts(normalize=True)\n",
        "        weight_left = len(left) / overall\n",
        "        gini_left = 1 - np.sum(np.square(props_left))\n",
        "\n",
        "        # Calculate the Gini for the right subset\n",
        "        props_right = right[target].value_counts(normalize=True)\n",
        "        weight_right = len(right) / overall\n",
        "        gini_right = 1 - np.sum(np.square(props_right))\n",
        "\n",
        "        # Combine Gini values and append to the list\n",
        "        gini = weight_left * gini_left + weight_right * gini_right\n",
        "        giniDict[val] = gini\n",
        "\n",
        "    best_split = min(giniDict, key=giniDict.get) # Find the corresponding split point\n",
        "    min_gini = giniDict[best_split]  # Get the minimum Gini value\n",
        "\n",
        "    return min_gini, best_split"
      ],
      "metadata": {
        "id": "4J0RZ5jTsGZG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to check if a feature is categorical\n",
        "def isCategorical(df, feature):\n",
        "    return df[feature].nunique() < 8  # A feature is considered categorical if it has fewer than 8 unique values\n"
      ],
      "metadata": {
        "id": "ktDb7968sJQ_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main attribute selection method function\n",
        "def attribute_selection_method(df, target, measure):\n",
        "    categoricalDict = {}  # Store categorical features {feature, result}\n",
        "    quantitativeDict = {} # Store quantitative features {feature, (result, split)}\n",
        "    features = df.columns.drop(target)  # Get all features except the target\n",
        "\n",
        "    for feature in features:\n",
        "        if isCategorical(df, feature):  # If feature is categorical\n",
        "            if measure == 'entropy':\n",
        "                result = categorical_entropy(df, target, feature)\n",
        "            elif measure == 'gini':\n",
        "                result = categorical_gini(df, target, feature)\n",
        "            categoricalDict[feature] = result\n",
        "        else:  # If feature is quantitative\n",
        "            if measure == 'entropy':\n",
        "                min, best_split = quantitative_entropy(df, target, feature)\n",
        "            elif measure == 'gini':\n",
        "                min, best_split = quantitative_gini(df, target, feature)\n",
        "            quantitativeDict[feature] = (min, best_split)\n",
        "\n",
        "    # Determine the best feature and split point\n",
        "    if categoricalDict and min(categoricalDict.values()) <= min(result for result, _ in quantitativeDict.values()):  # Extracts results from\n",
        "        return min(categoricalDict, key=categoricalDict.get)                                         # the tuple and finds the minimum value\n",
        "    else:\n",
        "        best_feature = min(quantitativeDict, key=lambda x: quantitativeDict[x][0]) # Returns the key associated with the smallest result\n",
        "        return best_feature, quantitativeDict[best_feature][1]"
      ],
      "metadata": {
        "id": "8f_5-V-6sL8t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mxitDf_8sRYJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mean Squared Error"
      ],
      "metadata": {
        "id": "l36Mu2a5se3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Squared Error for regression tasks\n",
        "def regression_mse(df, target, feature):\n",
        "    mseList = []\n",
        "    splitPoints = []\n",
        "    uniqueVals = np.sort(df[feature].unique())\n",
        "    overall = len(df)\n",
        "\n",
        "    for val in uniqueVals:\n",
        "        # Split into left and right\n",
        "        left = df[df[feature] <= val][target]\n",
        "        right = df[df[feature] > val][target]\n",
        "\n",
        "        # Calculate MSE for left and right subsets\n",
        "        if len(left) > 0:\n",
        "            mse_left = np.mean((left - left.mean()) ** 2)\n",
        "        else:\n",
        "            mse_left = 0\n",
        "\n",
        "        if len(right) > 0:\n",
        "            mse_right = np.mean((right - right.mean()) ** 2)\n",
        "        else:\n",
        "            mse_right = 0\n",
        "\n",
        "        # Weighted average of the MSEs\n",
        "        weight_left = len(left) / overall\n",
        "        weight_right = len(right) / overall\n",
        "        mse = weight_left * mse_left + weight_right * mse_right\n",
        "\n",
        "        mseList.append(mse)\n",
        "        splitPoints.append(val)\n",
        "\n",
        "    min_mse = np.min(mseList)\n",
        "    best_split = splitPoints[np.argmin(mseList)]\n",
        "    return min_mse, best_split"
      ],
      "metadata": {
        "id": "jTiWZOYkwuq5"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}